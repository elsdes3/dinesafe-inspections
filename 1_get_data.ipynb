{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81ac8fb1-1d0c-46f4-ab34-aed99a23ce3b",
   "metadata": {},
   "source": [
    "# Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fdecae7-a36d-479c-ab9d-8f6669929e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "306d0d56-45c1-4aa1-97cc-8a49f72a3867",
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "import os\n",
    "import shutil\n",
    "from glob import glob\n",
    "from io import BytesIO\n",
    "from zipfile import ZipFile\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea786505-252c-4c67-be88-ceb8f561f7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read(\"../sql.ini\")\n",
    "default_cfg = config[\"default\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0abe2ec3-b2b7-461a-b06e-7b6f03d17148",
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_TYPE = default_cfg[\"DB_TYPE\"]\n",
    "DB_DRIVER = default_cfg[\"DB_DRIVER\"]\n",
    "DB_USER = default_cfg[\"DB_USER\"]\n",
    "DB_PASS = default_cfg[\"DB_PASS\"]\n",
    "DB_HOST = default_cfg[\"DB_HOST\"]\n",
    "DB_PORT = default_cfg[\"DB_PORT\"]\n",
    "DB_NAME = default_cfg[\"DB_NAME\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f156844c-26ea-4ae6-86e0-4a521feb2357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to single database (required to create database)\n",
    "URI_NO_DB = f\"{DB_TYPE}+{DB_DRIVER}://{DB_USER}:{DB_PASS}@{DB_HOST}:{DB_PORT}\"\n",
    "\n",
    "# Connect to all databases (required to perform CRUD operations and submit queries)\n",
    "URI = f\"{DB_TYPE}+{DB_DRIVER}://{DB_USER}:{DB_PASS}@{DB_HOST}:{DB_PORT}/{DB_NAME}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5bfd57-e415-4f20-a510-1dc3fbbd2a49",
   "metadata": {},
   "source": [
    "## About"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0767a56e-8a7a-474f-9acc-977846ed6e42",
   "metadata": {},
   "source": [
    "In this notebook, we will download historical [Dinesafe inspections data](https://open.toronto.ca/dataset/dinesafe/) from WayBackMachine (internet web archive, [link](https://archive.org/)). These datasets are snapshots captured at various timestamps. We need these [snapshots](https://web.archive.org/web/*/http://opendata.toronto.ca/public.health/dinesafe/dinesafe.zip) since the version of this data on the Toronto Open Data portal covers a short period of time (approx. 18 months starting in Jan 2020). We will want to have access to as much data as possible to train an ML model to predict a critical infraction during future inspections.\n",
    "\n",
    "All historical datasets will be processed (dropping any inspections that might be duplicated across multiple snapshots), concatenated and then appended to a local MySQL database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e5de32-2981-4eaf-b31a-e2aa32655ceb",
   "metadata": {},
   "source": [
    "## Database Administration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d474a24b-61a9-4630-a696-8cae656b7001",
   "metadata": {},
   "source": [
    "The inspections data will be stored locally in a MySQL database. We'll first create the `dinesafe` database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5ddcf14-7453-420f-9a83-da6e399fc494",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine(URI_NO_DB)\n",
    "conn = engine.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db1cfdd5-e94e-4f30-a0dd-7839c5930d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = conn.execute(f\"DROP DATABASE IF EXISTS {DB_NAME};\")\n",
    "_ = conn.execute(f\"CREATE DATABASE IF NOT EXISTS {DB_NAME};\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cedb48fc-3b2b-46e3-916a-a2662bd97acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.close()\n",
    "engine.dispose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7569319e-49ac-4375-898d-efaaff3503c2",
   "metadata": {},
   "source": [
    "## Create Database Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87cee095-4a3b-4d15-bf07-f5280aac18b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine(URI)\n",
    "conn = engine.connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504201fa-9693-459c-abbd-fde9df26c7b4",
   "metadata": {},
   "source": [
    "Create the `inspections` table in the `dinesafe` database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71cbeb9-7198-466f-8c3b-e4942489ab57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of database table\n",
    "table_name = \"inspections\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d92f2d7-9a7c-4d42-a2ea-db6fc4e47120",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = conn.execute(f\"DROP TABLE IF EXISTS {table_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff87f918-f014-407e-afab-71a3cdcdd152",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_table_query = f\"\"\"\n",
    "                     CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "                         row_id INT,\n",
    "                         establishment_id INT,\n",
    "                         inspection_id INT,\n",
    "                         establishment_name TEXT,\n",
    "                         establishmenttype TEXT,\n",
    "                         establishment_address TEXT,\n",
    "                         latitude FLOAT,\n",
    "                         longitude FLOAT,\n",
    "                         establishment_status TEXT,\n",
    "                         minimum_inspections_peryear INT,\n",
    "                         infraction_details TEXT,\n",
    "                         inspection_date DATE,\n",
    "                         severity TEXT,\n",
    "                         action TEXT,\n",
    "                         court_outcome TEXT,\n",
    "                         amount_fined FLOAT\n",
    "                     )\n",
    "                     \"\"\"\n",
    "_ = conn.execute(create_table_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "03df554a-5e06-4e96-b209-1b3ee6f9bbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.close()\n",
    "engine.dispose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab261f9-0503-43b5-832c-4841aa284579",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Get Data and Populate Database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444f1d1e-e8b5-4d60-b8c6-b56228b25c3c",
   "metadata": {},
   "source": [
    "Retrieve DineSafe program data snapshots from WayBackMachine and append to the `dinesafe` database (drop duplicate inspections and change data types before appending to table)\n",
    "- a helper function `process_data()` is used to\n",
    "  - download historical inspections data from WayBackMachine (`extract()`) and unzip contents into `data/raw`\n",
    "  - process the raw inspections data (`process_data()`)\n",
    "    - change `INSPECTION_DATE` to a `datetime`\n",
    "    - change `MINIMUM_INSPECTIONS_PERYEAR` to an integer datatype\n",
    "    - clean `AMOUNT_FINED` (remove commas) and convert to numerical datatype\n",
    "    - append `LATITUDE` and `LONGITUDE` columns (if not present)\n",
    "      - some inspections data files have these but others don't\n",
    "      - since we'll be appending all files to the same database table, they will all need to have these columns even if the columns contain missing values\n",
    "    - append `LATITUDE` and `LONGITUDE` columns to lowercase\n",
    "  - append the processed data to the `inspections` table in the `dinesafe` database (`load()`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cee0d030-2879-4e37-8a68-440eb4f322df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(zip_filenames):\n",
    "    \"\"\"Retrieve dinesafe data snapshot XML files from WayBackMachine.\"\"\"\n",
    "    available_files = []\n",
    "    for zip_fname in zip_filenames:\n",
    "        # Assemble source URL\n",
    "        url = (\n",
    "            f\"https://web.archive.org/web/{zip_fname}/\"\n",
    "            \"http://opendata.toronto.ca/public.health/dinesafe/dinesafe.zip\"\n",
    "        )\n",
    "        # Create path to target dir, where extracted .XML file will be found\n",
    "        target_dir = f\"data/raw/{zip_fname}\"\n",
    "        if not os.path.exists(f\"data/raw/{zip_fname}/dinesafe.xml\"):\n",
    "            # Get zipped file containing .XML file\n",
    "            r = requests.get(url)\n",
    "            # Extract to target dir\n",
    "            with ZipFile(BytesIO(r.content)) as zfile:\n",
    "                zfile.extractall(target_dir)\n",
    "        available_files.append(target_dir)\n",
    "    return available_files\n",
    "\n",
    "\n",
    "def read_data(filepath):\n",
    "    \"\"\"Load an XML file into a DataFrame.\"\"\"\n",
    "    return pd.read_xml(filepath)\n",
    "\n",
    "\n",
    "def process_data(df, cols_order_wanted):\n",
    "    \"\"\"Process inspections data.\"\"\"\n",
    "    # Datetime formatting\n",
    "    df[\"INSPECTION_DATE\"] = pd.to_datetime(df[\"INSPECTION_DATE\"])\n",
    "    # Change datatype 1/2\n",
    "    df = df.astype({\"MINIMUM_INSPECTIONS_PERYEAR\": int})\n",
    "    # Remove commas from column and convert string to float\n",
    "    if df[\"AMOUNT_FINED\"].dtype == \"object\":\n",
    "        df[\"AMOUNT_FINED\"] = pd.to_numeric(\n",
    "            df[\"AMOUNT_FINED\"].astype(str).str.replace(\",\", \"\"), errors=\"coerce\"\n",
    "        )\n",
    "    # Append latitude and longitude columns, if not found in the data\n",
    "    for loc_col in [\"LATITUDE\", \"LONGITUDE\"]:\n",
    "        if loc_col not in list(df):\n",
    "            df[loc_col] = None\n",
    "    # Change column names to lowercase, Re-order columns and Change datatype of the\n",
    "    # latitude and longitude columns\n",
    "    df = df.rename(columns=str.lower)[cols_order_wanted].astype(\n",
    "        {\"latitude\": float, \"longitude\": float}\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "def transform(available_files, cols_order_wanted):\n",
    "    \"\"\"Transform data in downloaded XML files.\"\"\"\n",
    "    dfs = []\n",
    "    for f in available_files:\n",
    "        df = read_data(f\"{f}/dinesafe.xml\")\n",
    "        df = process_data(df, cols_order_wanted)\n",
    "        dfs.append(df)\n",
    "    return dfs\n",
    "\n",
    "\n",
    "def load(dfs, uri, table_name=\"inspections\"):\n",
    "    \"\"\"Vertically concatenate list of DataFrames and Append to database.\"\"\"\n",
    "    dfs_all = pd.concat(dfs, ignore_index=True).drop_duplicates(\n",
    "        keep=\"first\", subset=None\n",
    "    )\n",
    "    engine = create_engine(uri)\n",
    "    conn = engine.connect()\n",
    "    dfs_all.to_sql(name=table_name, con=conn, index=False, if_exists=\"append\")\n",
    "    conn.close()\n",
    "    engine.dispose()\n",
    "\n",
    "\n",
    "def retrieve_data(zip_filenames, uri, cols_order_wanted, table_name=\"inspections\"):\n",
    "    \"\"\"Retrieve data, process and append to database table.\"\"\"\n",
    "    # Extract\n",
    "    available_files_list = extract(zip_filenames)\n",
    "\n",
    "    # Transform\n",
    "    dfs = transform(available_files_list, cols_order_wanted)\n",
    "\n",
    "    # Load\n",
    "    load(dfs, uri, table_name)\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35963dde-977d-466c-b34e-13d377b415e9",
   "metadata": {},
   "source": [
    "Run the ETL workflow to retrieve historical inspections data files, process each file and append processed data to the `inspections` table of the `dinesafe` database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62c6a363-67c3-421a-88bd-02541e6a5448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data file names to download (these are timestamps at which data snapshot was\n",
    "# captured by WayBackMachine)\n",
    "zip_filenames = [\n",
    "    \"20130723222156\",\n",
    "    \"20150603085055\",\n",
    "    \"20151012004454\",\n",
    "    \"20160129205023\",\n",
    "    \"20160317045436\",\n",
    "    \"20160915001010\",\n",
    "    \"20170303162206\",\n",
    "    \"20170330001043\",\n",
    "    \"20170726115444\",\n",
    "    \"20190116215713\",\n",
    "    \"20190126084933\",\n",
    "    \"20190614092848\",\n",
    "    \"20210626163552\",\n",
    "]\n",
    "\n",
    "# Order of DataFrame columns (to re-order raw data) in order to match column order in database table\n",
    "cols_order_wanted = [\n",
    "    \"row_id\",\n",
    "    \"establishment_id\",\n",
    "    \"inspection_id\",\n",
    "    \"establishment_name\",\n",
    "    \"establishmenttype\",\n",
    "    \"establishment_address\",\n",
    "    \"latitude\",\n",
    "    \"longitude\",\n",
    "    \"establishment_status\",\n",
    "    \"minimum_inspections_peryear\",\n",
    "    \"infraction_details\",\n",
    "    \"inspection_date\",\n",
    "    \"severity\",\n",
    "    \"action\",\n",
    "    \"court_outcome\",\n",
    "    \"amount_fined\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "50720cac-85d3-4071-94e3-ca8c27ed825a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 30s, sys: 2.97 s, total: 1min 33s\n",
      "Wall time: 1min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dfs = retrieve_data(zip_filenames, URI, cols_order_wanted, table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f64498-ba11-4578-a536-16f4f4bdc37e",
   "metadata": {},
   "source": [
    "These datasets list each infraction for a single inspection on a separate row. We will now need to filter these infractions to only select relevant ones and then aggregate them by inspection, since each row (inspection) will be used as an independent observation by the ML model we train later.\n",
    "\n",
    "In the next notebook (`2_sql_filter_transform.ipynb`), we will filter these infractions and aggregate them by inspection."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
