{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81ac8fb1-1d0c-46f4-ab34-aed99a23ce3b",
   "metadata": {},
   "source": [
    "# Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4022120-60f3-4420-8779-ed281bef2c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306d0d56-45c1-4aa1-97cc-8a49f72a3867",
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "import os\n",
    "import shutil\n",
    "from glob import glob\n",
    "from io import BytesIO\n",
    "from zipfile import ZipFile\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea786505-252c-4c67-be88-ceb8f561f7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read(\"../sql.ini\")\n",
    "default_cfg = config[\"default\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abe2ec3-b2b7-461a-b06e-7b6f03d17148",
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_TYPE = default_cfg[\"DB_TYPE\"]\n",
    "DB_DRIVER = default_cfg[\"DB_DRIVER\"]\n",
    "DB_USER = default_cfg[\"DB_USER\"]\n",
    "DB_PASS = default_cfg[\"DB_PASS\"]\n",
    "DB_HOST = default_cfg[\"DB_HOST\"]\n",
    "DB_PORT = default_cfg[\"DB_PORT\"]\n",
    "DB_NAME = default_cfg[\"DB_NAME\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f156844c-26ea-4ae6-86e0-4a521feb2357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to single database (required to create database)\n",
    "URI_NO_DB = f\"{DB_TYPE}+{DB_DRIVER}://{DB_USER}:{DB_PASS}@{DB_HOST}:{DB_PORT}\"\n",
    "\n",
    "# Connect to all databases (required to perform CRUD operations and submit queries)\n",
    "URI = f\"{DB_TYPE}+{DB_DRIVER}://{DB_USER}:{DB_PASS}@{DB_HOST}:{DB_PORT}/{DB_NAME}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7678ff75-0491-48ae-a4e9-9f7069278742",
   "metadata": {},
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d2aa3c-1cbb-4de9-95c1-8f188623c5b5",
   "metadata": {},
   "source": [
    "### Objective\n",
    "To use Machine Learning (ML) to predict the liklihood of a significant or crucial infraction in establishment (restaurant, grocery store or similar) inspections conducted by the City of Toronto's DineSafe Inspection system. Data collected by the DineSafe program are obtained from the city's open data portal.\n",
    "\n",
    "### Facts about DineSafe Program in Toronto\n",
    "The following are facts about the data based on the city of Toronto's [Open Data Portal page for DineSafe data](https://open.toronto.ca/dataset/dinesafe/) and [general info page for the DineSafe program](https://www.toronto.ca/community-people/health-wellness-care/health-programs-advice/food-safety/dinesafe/about-dinesafe/)\n",
    "1. A single inspection takes place on a specific date at a single establishment. An `inspection_id` should be unique for each inspection. An `establishment_id` should be unique for each establishment.\n",
    "2. An establishment chain (such as the [SUBWAY](https://en.wikipedia.org/wiki/Subway_(restaurant)) brand), can have multiple establishment locations.\n",
    "3. Each location can be inspected one or more times (usually more than once). So, a single `establishment_id` and `inspection_id` should be associated with a single `inspection_date`.\n",
    "4. One or more infractions can be recorded per inspection. In the inspections data, each infraction is listed on a single row. There can be multiple rows (infractions) per inspection.\n",
    "5. If a Significant infraction is detected, an [inspector returns within two days](https://www.toronto.ca/community-people/health-wellness-care/health-programs-advice/food-safety/dinesafe/dinesafe-infractions/) to re-inspect (follow-up inspection) the establishment. The current ML use-case will not use such re-inspections so these inspections will be removed from the data (later in this notebook)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df46eba-266c-4a29-aad4-a88aee46e6a9",
   "metadata": {},
   "source": [
    "### Implications for Current Use-Case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde576c1-9332-447c-966f-f45b34dd1157",
   "metadata": {},
   "source": [
    "For the current ML use-case, we require each *observation* to be an independent inspection with or without an infraction (crucial, significant or minor). We will then create a binary variable indicating whether the inspection resulted in a significant or crucial infraction (1) or not (0) since that is that label that the ML algorithm needs to predict. Significant or crucial infractions present a health hazard, while minor infractions only present minimal health risk. The ML model will not be predicting the outcome of follow-up inspections, but will only be trained to predict the outcome (if there was a significant or crucial infraction, or not) of the initial inspection.\n",
    "\n",
    "When exploring the data, we will need to take these considerations into account as well as the facts about the data mentioned above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3c564d-de02-4453-a20d-d9e8791012fc",
   "metadata": {},
   "source": [
    "### Assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5373ba-d7c8-42cf-bf41-473adcef3354",
   "metadata": {},
   "source": [
    "#### Inspection Schedule During the Out-of-Sample Period\n",
    "When an ML model is trained, before it is called to make predictions, it is assumed that the future inspection schedule (establishments and planned inspection dates) are known ahead of time. These do not need to be made available ot the inspector. However, they must be provided to a prediction service that calls the trained ML model to predict if these scheduled inspections will result in an infraction. The ML model will predict the likelohood of detecting a significant or crucial infraction will be detected during these scheduled inspections (at the scheduled establishments) *ahead of the date on which the scheduled inspection will occur*. So, (if sufficiently accurate) the ML model can predict the likelihood of a crucial or significant infraction, during a scheduled inspection, before an inspector conducts a scheduled inspection.\n",
    "\n",
    "#### Other Examples of Handling an Out-of-Sample Period\n",
    "In one of the previous ML-based studies, a out-of-sample period of time is pre-chosen. During this time, the inspections proceed as planned. At the end of this period, when the inspection data becomes available (establishment locations and inspection dates), the ML model is used to predict the likelihood of an infraction during those inspections. So, the ML model does not have to predict into the future since it is being evaluated against true inspection data in the past. The reason for this approach is that the the applicatoin in question was part of a pilot study to estimate the efficacy of such an approach.\n",
    "\n",
    "#### Implications for Current Work\n",
    "By comparison, in the current project and with an eye towards deploying such an ML model, we need to predict this likelihood ahead of time so that establishments with infractions are known before the day of an in-person inspection by an inspector. For this reason, we are assuming here that the inspection schedule (establishments and planned inspection dates) are known ahead of time. This also means that any predictors (ML features) we use must be known ahead of the out-of-sample period. For example, if weather is to be used then a forecast of the weather conditions (eg. temperature) during the out-of-sample period is required for dates and locations corresponding to the planned inspection schedule since we will need this weather data on the date when the predictions of all out-of-sample inspections are to be made."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5bfd57-e415-4f20-a510-1dc3fbbd2a49",
   "metadata": {},
   "source": [
    "## About"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0767a56e-8a7a-474f-9acc-977846ed6e42",
   "metadata": {},
   "source": [
    "In this notebook, we will download historical [Dinesafe inspections data](https://open.toronto.ca/dataset/dinesafe/) from WayBackMachine (internet web archive, [link](https://archive.org/)). These datasets are snapshots captured at various timestamps. We need these [snapshots](https://web.archive.org/web/*/http://opendata.toronto.ca/public.health/dinesafe/dinesafe.zip) since the version of this data on the Toronto Open Data portal covers a short period of time (approx. 18 months starting in Jan 2020). We will want to have access to as much data as possible to train an ML model to predict a critical infraction during future inspections.\n",
    "\n",
    "All historical datasets will be processed (dropping any inspections that might be duplicated across multiple snapshots), concatenated and then appended to a local MySQL database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e5de32-2981-4eaf-b31a-e2aa32655ceb",
   "metadata": {},
   "source": [
    "## Database Administration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d474a24b-61a9-4630-a696-8cae656b7001",
   "metadata": {},
   "source": [
    "The inspections data will be stored locally in a MySQL database. We'll first create the `dinesafe` database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ddcf14-7453-420f-9a83-da6e399fc494",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine(URI_NO_DB)\n",
    "conn = engine.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1cfdd5-e94e-4f30-a0dd-7839c5930d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = conn.execute(f\"DROP DATABASE IF EXISTS {DB_NAME};\")\n",
    "_ = conn.execute(f\"CREATE DATABASE IF NOT EXISTS {DB_NAME};\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedb48fc-3b2b-46e3-916a-a2662bd97acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.close()\n",
    "engine.dispose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7569319e-49ac-4375-898d-efaaff3503c2",
   "metadata": {},
   "source": [
    "## Create Database Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87cee095-4a3b-4d15-bf07-f5280aac18b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine(URI)\n",
    "conn = engine.connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504201fa-9693-459c-abbd-fde9df26c7b4",
   "metadata": {},
   "source": [
    "Create the `inspections` table in the `dinesafe` database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71cbeb9-7198-466f-8c3b-e4942489ab57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of database table\n",
    "table_name = \"inspections\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d92f2d7-9a7c-4d42-a2ea-db6fc4e47120",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = conn.execute(f\"DROP TABLE IF EXISTS {table_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff87f918-f014-407e-afab-71a3cdcdd152",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_table_query = f\"\"\"\n",
    "                     CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "                         row_id INT,\n",
    "                         establishment_id INT,\n",
    "                         inspection_id INT,\n",
    "                         establishment_name TEXT,\n",
    "                         establishmenttype TEXT,\n",
    "                         establishment_address TEXT,\n",
    "                         latitude FLOAT,\n",
    "                         longitude FLOAT,\n",
    "                         establishment_status TEXT,\n",
    "                         minimum_inspections_peryear INT,\n",
    "                         infraction_details TEXT,\n",
    "                         inspection_date DATE,\n",
    "                         severity TEXT,\n",
    "                         action TEXT,\n",
    "                         court_outcome TEXT,\n",
    "                         amount_fined FLOAT\n",
    "                     )\n",
    "                     \"\"\"\n",
    "_ = conn.execute(create_table_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03df554a-5e06-4e96-b209-1b3ee6f9bbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.close()\n",
    "engine.dispose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab261f9-0503-43b5-832c-4841aa284579",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Get Data and Populate Database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444f1d1e-e8b5-4d60-b8c6-b56228b25c3c",
   "metadata": {},
   "source": [
    "Retrieve DineSafe program data snapshots from WayBackMachine and append to the `dinesafe` database (drop duplicate inspections and change data types before appending to table)\n",
    "- a helper function `process_data()` is used to\n",
    "  - download historical inspections data from WayBackMachine (`extract()`) and unzip contents into `data/raw`\n",
    "  - process the raw inspections data (`process_data()`)\n",
    "    - change `INSPECTION_DATE` to a `datetime`\n",
    "    - change `MINIMUM_INSPECTIONS_PERYEAR` to an integer datatype\n",
    "    - clean `AMOUNT_FINED` (remove commas) and convert to numerical datatype\n",
    "    - append `LATITUDE` and `LONGITUDE` columns (if not present)\n",
    "      - some inspections data files have these but others don't\n",
    "      - since we'll be appending all files to the same database table, they will all need to have these columns even if the columns contain missing values\n",
    "    - append `LATITUDE` and `LONGITUDE` columns to lowercase\n",
    "  - append the processed data to the `inspections` table in the `dinesafe` database (`load()`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee0d030-2879-4e37-8a68-440eb4f322df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(zip_filenames):\n",
    "    \"\"\"Retrieve dinesafe data snapshot XML files from WayBackMachine.\"\"\"\n",
    "    available_files = []\n",
    "    for zip_fname in zip_filenames:\n",
    "        # Assemble source URL\n",
    "        url = (\n",
    "            f\"https://web.archive.org/web/{zip_fname}/\"\n",
    "            \"http://opendata.toronto.ca/public.health/dinesafe/dinesafe.zip\"\n",
    "        )\n",
    "        # Create path to target dir, where extracted .XML file will be found\n",
    "        target_dir = f\"data/raw/{zip_fname}\"\n",
    "        if not os.path.exists(f\"data/raw/{zip_fname}/dinesafe.xml\"):\n",
    "            # Get zipped file containing .XML file\n",
    "            r = requests.get(url)\n",
    "            # Extract to target dir\n",
    "            with ZipFile(BytesIO(r.content)) as zfile:\n",
    "                zfile.extractall(target_dir)\n",
    "        available_files.append(target_dir)\n",
    "    return available_files\n",
    "\n",
    "\n",
    "def read_data(filepath):\n",
    "    \"\"\"Load an XML file into a DataFrame.\"\"\"\n",
    "    return pd.read_xml(filepath)\n",
    "\n",
    "\n",
    "def process_data(df, cols_order_wanted):\n",
    "    \"\"\"Process inspections data.\"\"\"\n",
    "    # Datetime formatting\n",
    "    df[\"INSPECTION_DATE\"] = pd.to_datetime(df[\"INSPECTION_DATE\"])\n",
    "    # Change datatype 1/2\n",
    "    df = df.astype({\"MINIMUM_INSPECTIONS_PERYEAR\": int})\n",
    "    # Remove commas from column and convert string to float\n",
    "    if df[\"AMOUNT_FINED\"].dtype == \"object\":\n",
    "        df[\"AMOUNT_FINED\"] = pd.to_numeric(\n",
    "            df[\"AMOUNT_FINED\"].astype(str).str.replace(\",\", \"\"), errors=\"coerce\"\n",
    "        )\n",
    "    # Append latitude and longitude columns, if not found in the data\n",
    "    for loc_col in [\"LATITUDE\", \"LONGITUDE\"]:\n",
    "        if loc_col not in list(df):\n",
    "            df[loc_col] = None\n",
    "    # Change column names to lowercase, Re-order columns and Change datatype of the\n",
    "    # latitude and longitude columns\n",
    "    df = df.rename(columns=str.lower)[cols_order_wanted].astype(\n",
    "        {\"latitude\": float, \"longitude\": float}\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "def transform(available_files, cols_order_wanted):\n",
    "    \"\"\"Transform data in downloaded XML files.\"\"\"\n",
    "    dfs = []\n",
    "    for f in available_files:\n",
    "        df = read_data(f\"{f}/dinesafe.xml\")\n",
    "        df = process_data(df, cols_order_wanted)\n",
    "        dfs.append(df)\n",
    "    return dfs\n",
    "\n",
    "\n",
    "def load(dfs, uri, table_name=\"inspections\"):\n",
    "    \"\"\"Vertically concatenate list of DataFrames and Append to database.\"\"\"\n",
    "    dfs_all = pd.concat(dfs, ignore_index=True).drop_duplicates(\n",
    "        keep=\"first\", subset=None\n",
    "    )\n",
    "    engine = create_engine(uri)\n",
    "    conn = engine.connect()\n",
    "    dfs_all.to_sql(name=table_name, con=conn, index=False, if_exists=\"append\")\n",
    "    conn.close()\n",
    "    engine.dispose()\n",
    "\n",
    "\n",
    "def retrieve_data(zip_filenames, uri, cols_order_wanted, table_name=\"inspections\"):\n",
    "    \"\"\"Retrieve data, process and append to database table.\"\"\"\n",
    "    # Extract\n",
    "    available_files_list = extract(zip_filenames)\n",
    "\n",
    "    # Transform\n",
    "    dfs = transform(available_files_list, cols_order_wanted)\n",
    "\n",
    "    # Load\n",
    "    load(dfs, uri, table_name)\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35963dde-977d-466c-b34e-13d377b415e9",
   "metadata": {},
   "source": [
    "Run the ETL workflow to retrieve historical inspections data files, process each file and append processed data to the `inspections` table of the `dinesafe` database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c6a363-67c3-421a-88bd-02541e6a5448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data file names to download (these are timestamps at which data snapshot was\n",
    "# captured by WayBackMachine)\n",
    "zip_filenames = [\n",
    "    \"20130723222156\",\n",
    "    \"20150603085055\",\n",
    "    \"20151012004454\",\n",
    "    \"20160129205023\",\n",
    "    \"20160317045436\",\n",
    "    \"20160915001010\",\n",
    "    \"20170303162206\",\n",
    "    \"20170330001043\",\n",
    "    \"20170726115444\",\n",
    "    \"20190116215713\",\n",
    "    \"20190126084933\",\n",
    "    \"20190614092848\",\n",
    "    \"20210626163552\",\n",
    "]\n",
    "\n",
    "# Order of DataFrame columns (to re-order raw data) in order to match column order in database table\n",
    "cols_order_wanted = [\n",
    "    \"row_id\",\n",
    "    \"establishment_id\",\n",
    "    \"inspection_id\",\n",
    "    \"establishment_name\",\n",
    "    \"establishmenttype\",\n",
    "    \"establishment_address\",\n",
    "    \"latitude\",\n",
    "    \"longitude\",\n",
    "    \"establishment_status\",\n",
    "    \"minimum_inspections_peryear\",\n",
    "    \"infraction_details\",\n",
    "    \"inspection_date\",\n",
    "    \"severity\",\n",
    "    \"action\",\n",
    "    \"court_outcome\",\n",
    "    \"amount_fined\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50720cac-85d3-4071-94e3-ca8c27ed825a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "dfs = retrieve_data(zip_filenames, URI, cols_order_wanted, table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f64498-ba11-4578-a536-16f4f4bdc37e",
   "metadata": {},
   "source": [
    "These datasets list each infraction for a single inspection on a separate row. We will now need to filter these infractions to only select relevant ones and then aggregate them by inspection, since each row (inspection) will be used as an independent observation by the ML model we train later.\n",
    "\n",
    "In the next notebook (`2_sql_filter_transform.ipynb`), we will filter these infractions and aggregate them by inspection."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
