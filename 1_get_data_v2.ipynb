{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81ac8fb1-1d0c-46f4-ab34-aed99a23ce3b",
   "metadata": {},
   "source": [
    "# Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4022120-60f3-4420-8779-ed281bef2c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "306d0d56-45c1-4aa1-97cc-8a49f72a3867",
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "import os\n",
    "from glob import glob\n",
    "from io import BytesIO\n",
    "from typing import Dict, List, Union\n",
    "from zipfile import ZipFile\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "import snowflake.connector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd713384-e907-41e3-9f25-01cccc9aa2cc",
   "metadata": {},
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9588f07-7418-419b-963d-a01b821902e8",
   "metadata": {},
   "source": [
    "### Objective\n",
    "To use Machine Learning (ML) to predict the liklihood of a significant or crucial infraction in establishment (restaurant, grocery store or similar) inspections conducted by the City of Toronto's DineSafe Inspection system. Data collected by the DineSafe program are obtained from the city's open data portal.\n",
    "\n",
    "### Facts about DineSafe Program in Toronto\n",
    "The following are facts about the data based on the city of Toronto's [Open Data Portal page for DineSafe data](https://open.toronto.ca/dataset/dinesafe/) and [general info page for the DineSafe program](https://www.toronto.ca/community-people/health-wellness-care/health-programs-advice/food-safety/dinesafe/about-dinesafe/)\n",
    "1. A single inspection takes place on a specific date at a single establishment. An `inspection_id` should be unique for each inspection. An `establishment_id` should be unique for each establishment.\n",
    "2. An establishment chain (such as the [SUBWAY](https://en.wikipedia.org/wiki/Subway_(restaurant)) brand), can have multiple establishment locations.\n",
    "3. Each location can be inspected one or more times (usually more than once). So, a single `establishment_id` and `inspection_id` should be associated with a single `inspection_date`.\n",
    "4. One or more infractions can be recorded per inspection. In the inspections data, each infraction is listed on a single row. There can be multiple rows (infractions) per inspection.\n",
    "5. If a Significant infraction is detected, an [inspector returns within two days](https://www.toronto.ca/community-people/health-wellness-care/health-programs-advice/food-safety/dinesafe/dinesafe-infractions/) to re-inspect (follow-up inspection) the establishment. The current ML use-case will not use such re-inspections so these inspections will be removed from the data (later in this notebook)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542c350a-e4da-44d2-bcd4-4996c460b930",
   "metadata": {},
   "source": [
    "### Implications for Current Use-Case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ba4229-40f5-4ce1-a840-42af3a7aa8ff",
   "metadata": {},
   "source": [
    "For the current ML use-case, we require each *observation* to be an independent inspection with or without an infraction (crucial, significant or minor). We will then create a binary variable indicating whether the inspection resulted in a significant or crucial infraction (1) or not (0) since that is that label that the ML algorithm needs to predict. Significant or crucial infractions present a health hazard, while minor infractions only present minimal health risk. The ML model will not be predicting the outcome of follow-up inspections, but will only be trained to predict the outcome (if there was a significant or crucial infraction, or not) of the initial inspection.\n",
    "\n",
    "When exploring the data, we will need to take these considerations into account as well as the facts about the data mentioned above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f501d7b7-985a-40ef-8ffd-c10b4be9dac6",
   "metadata": {},
   "source": [
    "### Assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccdea633-a14e-44bd-82d0-637560fb3079",
   "metadata": {},
   "source": [
    "#### Inspection Schedule During the Out-of-Sample Period\n",
    "When an ML model is trained, before it is called to make predictions, it is assumed that the future inspection schedule (establishments and planned inspection dates) are known ahead of time. These do not need to be made available ot the inspector. However, they must be provided to a prediction service that calls the trained ML model to predict if these scheduled inspections will result in an infraction. The ML model will predict the likelohood of detecting a significant or crucial infraction will be detected during these scheduled inspections (at the scheduled establishments) *ahead of the date on which the scheduled inspection will occur*. So, (if sufficiently accurate) the ML model can predict the likelihood of a crucial or significant infraction, during a scheduled inspection, before an inspector conducts a scheduled inspection.\n",
    "\n",
    "#### Other Examples of Handling an Out-of-Sample Period\n",
    "In one of the previous ML-based studies, a out-of-sample period of time is pre-chosen. During this time, the inspections proceed as planned. At the end of this period, when the inspection data becomes available (establishment locations and inspection dates), the ML model is used to predict the likelihood of an infraction during those inspections. So, the ML model does not have to predict into the future since it is being evaluated against true inspection data in the past. The reason for this approach is that the the applicatoin in question was part of a pilot study to estimate the efficacy of such an approach.\n",
    "\n",
    "#### Implications for Current Work\n",
    "By comparison, in the current project and with an eye towards deploying such an ML model, we need to predict this likelihood ahead of time so that establishments with infractions are known before the day of an in-person inspection by an inspector. For this reason, we are assuming here that the inspection schedule (establishments and planned inspection dates) are known ahead of time. This also means that any predictors (ML features) we use must be known ahead of the out-of-sample period. For example, if weather is to be used then a forecast of the weather conditions (eg. temperature) during the out-of-sample period is required for dates and locations corresponding to the planned inspection schedule since we will need this weather data on the date when the predictions of all out-of-sample inspections are to be made."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92e8d79-5f38-4cbd-9b8a-779505b94c4a",
   "metadata": {},
   "source": [
    "## About"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d955cf96-6672-41f3-a45e-7093c05c184c",
   "metadata": {},
   "source": [
    "In this notebook, we will download historical [Dinesafe inspections data](https://open.toronto.ca/dataset/dinesafe/) from WayBackMachine (internet web archive, [link](https://archive.org/)). These datasets are snapshots captured at various timestamps. We need these [snapshots](https://web.archive.org/web/*/http://opendata.toronto.ca/public.health/dinesafe/dinesafe.zip) since the version of this data on the Toronto Open Data portal covers a short period of time (approx. 18 months starting in Jan 2020). We will want to have access to as much data as possible to train an ML model to predict a critical infraction during future inspections.\n",
    "\n",
    "All historical datasets will be processed (dropping any inspections that might be duplicated across multiple snapshots), concatenated and then appended to a local MySQL database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b46e88d-c566-45ec-9b27-f2d1d1f252b0",
   "metadata": {},
   "source": [
    "## User Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d71cbeb9-7198-466f-8c3b-e4942489ab57",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Name of database table\n",
    "table_name = \"inspections\"\n",
    "\n",
    "# Data file names to download (these are timestamps at which data snapshot was\n",
    "# captured by WayBackMachine)\n",
    "zip_filenames = [\n",
    "    \"20130723222156\",\n",
    "    \"20150603085055\",\n",
    "    \"20151012004454\",\n",
    "    \"20160129205023\",\n",
    "    \"20160317045436\",\n",
    "    \"20160915001010\",\n",
    "    \"20170303162206\",\n",
    "    \"20170330001043\",\n",
    "    \"20170726115444\",\n",
    "    \"20190116215713\",\n",
    "    \"20190126084933\",\n",
    "    \"20190614092848\",\n",
    "    \"20210626163552\",\n",
    "]\n",
    "\n",
    "stage_name = \"processed_dinesafe_data\"\n",
    "file_format_name = \"COMMACOLSEP_ONEHEADROW\"\n",
    "\n",
    "num_proc_data_files = 10\n",
    "\n",
    "ci_run = \"no\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0abe2ec3-b2b7-461a-b06e-7b6f03d17148",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ci_run == \"yes\":\n",
    "    ACCOUNT = os.getenv(\"SNOWFLAKE_ACCOUNT\")\n",
    "    USER = os.getenv(\"SNOWFLAKE_USER\")\n",
    "    PASS = os.getenv(\"SNOWFLAKE_PASS\")\n",
    "    WAREHOUSE = os.getenv(\"SNOWFLAKE_WAREHOUSE\")\n",
    "    DB_SCHEMA = os.getenv(\"SNOWFLAKE_DB_SCHEMA\")\n",
    "    DB_NAME = \"dinesafe\"\n",
    "else:\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read(\"../sql.ini\")\n",
    "    default_cfg = config[\"default\"]\n",
    "    ACCOUNT = default_cfg[\"SNOWFLAKE_ACCOUNT\"]\n",
    "    USER = default_cfg[\"SNOWFLAKE_USER\"]\n",
    "    PASS = default_cfg[\"SNOWFLAKE_PASS\"]\n",
    "    WAREHOUSE = default_cfg[\"SNOWFLAKE_WAREHOUSE\"]\n",
    "    DB_SCHEMA = default_cfg[\"SNOWFLAKE_DB_SCHEMA\"]\n",
    "    DB_NAME = \"dinesafe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "216543ff-7b6f-4de8-86bf-b2c57e01c57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "connector_dict = dict(\n",
    "    account=ACCOUNT,\n",
    "    user=USER,\n",
    "    password=PASS,\n",
    "    database=DB_NAME,\n",
    "    schema=\"public\",\n",
    "    warehouse=WAREHOUSE,\n",
    "    role=\"sysadmin\",\n",
    ")\n",
    "connector_dict_no_db = dict(\n",
    "    account=ACCOUNT,\n",
    "    user=USER,\n",
    "    password=PASS,\n",
    "    warehouse=WAREHOUSE,\n",
    "    role=\"sysadmin\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62f7d937-76e0-4b9d-8707-e0ff93fe94ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_sql_df(\n",
    "    query: str,\n",
    "    cursor,\n",
    "    cnx=None,\n",
    "    table_output: bool = False,\n",
    "    use_manual_approach: bool = False,\n",
    ") -> Union[None, pd.DataFrame]:\n",
    "    cursor.execute(query)\n",
    "    if cnx:\n",
    "        cnx.commit()\n",
    "    if table_output:\n",
    "        if use_manual_approach:\n",
    "            colnames = [cdesc[0].lower() for cdesc in cursor.description]\n",
    "            cur_fetched = cursor.fetchall()\n",
    "            if cur_fetched:\n",
    "                df_query_output = pd.DataFrame.from_records(\n",
    "                    cur_fetched, columns=colnames\n",
    "                )\n",
    "                with pd.option_context(\n",
    "                    \"display.max_columns\", 200, \"display.max_colwidth\", 200\n",
    "                ):\n",
    "                    display(df_query_output)\n",
    "                return df_query_output\n",
    "        else:\n",
    "            df_query_output = cursor.fetch_pandas_all()\n",
    "            with pd.option_context(\n",
    "                \"display.max_columns\", 200, \"display.max_colwidth\", 200\n",
    "            ):\n",
    "                display(df_query_output)\n",
    "            return df_query_output\n",
    "    return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47463135-100d-4c41-bcc0-092fa9d194f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_database(connector_dict_no_db: Dict, database_name: str) -> None:\n",
    "    conn = snowflake.connector.connect(**connector_dict_no_db)\n",
    "    cur = conn.cursor()\n",
    "    for query in [\n",
    "        f\"DROP DATABASE IF EXISTS {database_name}\",\n",
    "        f\"CREATE DATABASE IF NOT EXISTS {database_name}\",\n",
    "        f\"COMMENT ON DATABASE {database_name} IS 'Toronto dinesafe inspections'\",\n",
    "    ]:\n",
    "        _ = cur.execute(query)\n",
    "    query = f\"\"\"\n",
    "            SHOW DATABASES LIKE '%{database_name}%'\n",
    "            \"\"\"\n",
    "    df = show_sql_df(query, cur, conn, True, True)\n",
    "    assert database_name in df[\"name\"].str.lower().tolist()\n",
    "    print(f\"Created database {database_name}\")\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "def create_table(connector_dict: Dict, table_name: str) -> None:\n",
    "    conn = snowflake.connector.connect(**connector_dict)\n",
    "    cur = conn.cursor()\n",
    "    for query in [\n",
    "        f\"DROP TABLE IF EXISTS {table_name}\",\n",
    "        f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "            ROW_ID INT,\n",
    "            ESTABLISHMENT_ID INT,\n",
    "            INSPECTION_ID INT,\n",
    "            ESTABLISHMENT_NAME TEXT,\n",
    "            ESTABLISHMENTTYPE TEXT,\n",
    "            ESTABLISHMENT_ADDRESS TEXT,\n",
    "            ESTABLISHMENT_STATUS TEXT,\n",
    "            MINIMUM_INSPECTIONS_PERYEAR INT,\n",
    "            INFRACTION_DETAILS TEXT,\n",
    "            INSPECTION_DATE DATE,\n",
    "            SEVERITY TEXT,\n",
    "            ACTION TEXT,\n",
    "            COURT_OUTCOME TEXT,\n",
    "            AMOUNT_FINED FLOAT,\n",
    "            LATITUDE FLOAT,\n",
    "            LONGITUDE FLOAT\n",
    "        )\n",
    "        \"\"\",\n",
    "        f\"COMMENT ON TABLE {table_name} IS 'Toronto Dinesafe inspections data'\",\n",
    "    ]:\n",
    "        _ = cur.execute(query)\n",
    "    query = f\"\"\"\n",
    "            SHOW TABLES LIKE '%{table_name}%'\n",
    "            \"\"\"\n",
    "    df = show_sql_df(query, cur, conn, True, True)\n",
    "    assert table_name in df[\"name\"].str.lower().tolist()\n",
    "    print(f\"Created table {table_name}\")\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "def create_file_format(connector_dict: Dict, file_format_name: str) -> None:\n",
    "    conn = snowflake.connector.connect(**connector_dict)\n",
    "    cur = conn.cursor()\n",
    "    for query in [\n",
    "        f\"DROP FILE FORMAT IF EXISTS {file_format_name}\",\n",
    "        rf\"\"\"\n",
    "        CREATE OR REPLACE FILE FORMAT {file_format_name}\n",
    "        TYPE = 'CSV'\n",
    "        COMPRESSION = 'AUTO'\n",
    "        FIELD_DELIMITER = ','\n",
    "        RECORD_DELIMITER = '\\\\n'\n",
    "        SKIP_HEADER = 1\n",
    "        FIELD_OPTIONALLY_ENCLOSED_BY='\"'\n",
    "        ERROR_ON_COLUMN_COUNT_MISMATCH = TRUE\n",
    "        ESCAPE = 'NONE'\n",
    "        ESCAPE_UNENCLOSED_FIELD = '\\134'\n",
    "        DATE_FORMAT = 'AUTO'\n",
    "        NULL_IF = ('\\\\N')\n",
    "        \"\"\",\n",
    "        (\n",
    "            f\"COMMENT ON FILE FORMAT {file_format_name} IS \"\n",
    "            \"'file format for Toronto dinesafe processed data files'\"\n",
    "        ),\n",
    "    ]:\n",
    "        _ = cur.execute(query)\n",
    "    query = f\"\"\"\n",
    "            SHOW FILE FORMATS LIKE '%{file_format_name}%'\n",
    "            \"\"\"\n",
    "    df = show_sql_df(query, cur, conn, True, True)\n",
    "    assert file_format_name in df[\"name\"].tolist()\n",
    "    print(f\"Created file format {file_format_name}\")\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "def create_stage(connector_dict: Dict, stage_name: str, file_format_name: str) -> None:\n",
    "    conn = snowflake.connector.connect(**connector_dict)\n",
    "    cur = conn.cursor()\n",
    "    for query in [\n",
    "        f\"DROP STAGE IF EXISTS {stage_name}\",\n",
    "        f\"\"\"\n",
    "        CREATE OR REPLACE STAGE {stage_name}\n",
    "        FILE_FORMAT = {file_format_name}\n",
    "        \"\"\",\n",
    "        (\n",
    "            f\"COMMENT ON STAGE {stage_name} IS \"\n",
    "            \"'local dir for Toronto dinesafe processed data'\"\n",
    "        ),\n",
    "    ]:\n",
    "        _ = cur.execute(query)\n",
    "    query = f\"\"\"\n",
    "            SHOW STAGES LIKE '%{stage_name}%'\n",
    "            \"\"\"\n",
    "    df = show_sql_df(query, cur, conn, True, True)\n",
    "    assert stage_name in df[\"name\"].str.lower().tolist()\n",
    "    print(f\"Created stage {stage_name}\")\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "def add_processed_data_to_stage(\n",
    "    connector_dict: Dict,\n",
    "    stage_name: str,\n",
    ") -> None:\n",
    "    conn = snowflake.connector.connect(**connector_dict)\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    processed_filepaths = glob(\"data/processed/*.csv\")\n",
    "    for processed_filepath in processed_filepaths:\n",
    "        query = f\"\"\"\n",
    "                PUT file://{processed_filepath} @{stage_name}\n",
    "                \"\"\"\n",
    "        print(f\"{query.strip()}...\", end=\"\")\n",
    "        _ = cur.execute(query)\n",
    "        print(\"Done.\")\n",
    "\n",
    "    query = f\"\"\"\n",
    "            LIST @{stage_name}/\n",
    "            \"\"\"\n",
    "    df = show_sql_df(query, cur, conn, True, True)\n",
    "    assert df.shape[0] == len(processed_filepaths)\n",
    "    print(\n",
    "        f\"Added {len(processed_filepaths):,} processed data files to stage {stage_name}\"\n",
    "    )\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "def copy_staged_data_to_table(\n",
    "    connector_dict: Dict,\n",
    "    table_name: str,\n",
    "    stage_name: str,\n",
    "    expected_num_rows: int,\n",
    ") -> None:\n",
    "    conn = snowflake.connector.connect(**connector_dict)\n",
    "    cur = conn.cursor()\n",
    "    query = f\"\"\"\n",
    "            COPY INTO {table_name} from @{stage_name}\n",
    "            \"\"\"\n",
    "    _ = cur.execute(query)\n",
    "\n",
    "    query = f\"\"\"\n",
    "            SELECT COUNT(*) AS num_rows\n",
    "            FROM {table_name}\n",
    "            \"\"\"\n",
    "    df = show_sql_df(query, cur, conn, True, True)\n",
    "    assert df.loc[0, \"num_rows\"] == expected_num_rows\n",
    "    print(\n",
    "        f\"Copied {len(df):,} rows of processed data from \"\n",
    "        f\"stage {stage_name} to table {table_name}\"\n",
    "    )\n",
    "    cur.close()\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4960d7a1-88b7-418a-8c79-1848835a650b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(zip_fname: str) -> List[str]:\n",
    "    \"\"\"Retrieve dinesafe data snapshot XML file from WayBackMachine.\"\"\"\n",
    "    # Assemble source URL\n",
    "    url = (\n",
    "        f\"https://web.archive.org/web/{zip_fname}/\"\n",
    "        \"http://opendata.toronto.ca/public.health/dinesafe/dinesafe.zip\"\n",
    "    )\n",
    "    # Create path to target dir, where extracted .XML file will be found\n",
    "    target_local_dir = f\"data/raw/{zip_fname}\"\n",
    "    target_local_filepath = f\"{target_local_dir}/dinesafe.xml\"\n",
    "    if not os.path.exists(target_local_filepath):\n",
    "        print(f\"Downloading data from {zip_fname}...\", end=\"\")\n",
    "        # Get zipped file containing .XML file\n",
    "        r = requests.get(url)\n",
    "        # Extract to target dir\n",
    "        with ZipFile(BytesIO(r.content)) as zfile:\n",
    "            zfile.extractall(target_local_dir)\n",
    "    else:\n",
    "        print(f\"Found local data for {zip_fname}...\", end=\"\")\n",
    "    return target_local_dir\n",
    "\n",
    "\n",
    "def read_data(filepath: str) -> pd.DataFrame:\n",
    "    \"\"\"Load an XML file into a DataFrame.\"\"\"\n",
    "    return pd.read_xml(filepath)\n",
    "\n",
    "\n",
    "def process_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Process inspections data.\"\"\"\n",
    "    # Datetime formatting\n",
    "    df[\"INSPECTION_DATE\"] = pd.to_datetime(df[\"INSPECTION_DATE\"])\n",
    "\n",
    "    # Change datatype 1/2\n",
    "    df = df.astype({\"MINIMUM_INSPECTIONS_PERYEAR\": int})\n",
    "\n",
    "    # Remove commas from column and convert string to float\n",
    "    if df[\"AMOUNT_FINED\"].dtype == \"object\":\n",
    "        df[\"AMOUNT_FINED\"] = pd.to_numeric(\n",
    "            df[\"AMOUNT_FINED\"].astype(str).str.replace(\",\", \"\"), errors=\"coerce\"\n",
    "        )\n",
    "\n",
    "    # Append latitude and longitude columns with missing values (if not found in data)\n",
    "    for loc_col in [\"LATITUDE\", \"LONGITUDE\"]:\n",
    "        if loc_col not in list(df):\n",
    "            df[loc_col] = None\n",
    "\n",
    "    # Change datatype 2/2\n",
    "    df = df.astype({\"LATITUDE\": float, \"LONGITUDE\": float})\n",
    "    return df\n",
    "\n",
    "\n",
    "def transform(raw_data_dir: str) -> pd.DataFrame:\n",
    "    \"\"\"Transform local data in downloaded XML file.\"\"\"\n",
    "    df = read_data(f\"{raw_data_dir}/dinesafe.xml\")\n",
    "    print(f\"Processing data from {raw_data_dir}...\", end=\"\")\n",
    "    df = process_data(df)\n",
    "    print(\"Done.\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def load(\n",
    "    dfs: List[pd.DataFrame],\n",
    "    connector_dict: Dict[str, str],\n",
    "    num_proc_data_files: int,\n",
    "    stage_name: str,\n",
    "    table_name: str,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Vertically concatenate list of DataFrames and Append to database.\"\"\"\n",
    "    df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    # Drop full rows that are exact duplicates of other rows\n",
    "    df = df.drop_duplicates(keep=\"first\", subset=None).reset_index(drop=True)\n",
    "\n",
    "    # Add file counter column\n",
    "    num_rows_per_csv_file = len(df) / num_proc_data_files\n",
    "    print(f\"Will split processed data among {num_proc_data_files:,} CSV files.\")\n",
    "    df[\"file_number\"] = (df.index // num_rows_per_csv_file).astype(int)\n",
    "\n",
    "    # Export concatenated DataFrame to CSV files\n",
    "    for csv_idx in range(df[\"file_number\"].max() + 1):\n",
    "        proc_fpath = f\"data/processed/dinesafe_{csv_idx}.csv\"\n",
    "        if not os.path.exists(proc_fpath):\n",
    "            df.query(f\"file_number == {csv_idx}\").drop(columns=[\"file_number\"]).to_csv(\n",
    "                proc_fpath,\n",
    "                index=False,\n",
    "            )\n",
    "            print(f\"Exported processed data to CSV file index {csv_idx}.\")\n",
    "        else:\n",
    "            print(f\"Found CSV file index {csv_idx} with processed data. Did nothing.\")\n",
    "\n",
    "    # Append to table in database\n",
    "    add_processed_data_to_stage(connector_dict, stage_name)\n",
    "    copy_staged_data_to_table(connector_dict, table_name, stage_name, len(df))\n",
    "    return df\n",
    "\n",
    "\n",
    "def retrieve_data(\n",
    "    zip_filenames: List[str],\n",
    "    connector_dict: Dict[str, str],\n",
    "    num_proc_data_files: int,\n",
    "    stage_name: str,\n",
    "    table_name: str,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Retrieve data, process and append to database table.\"\"\"\n",
    "    dfs = []\n",
    "    for zip_filename in zip_filenames:\n",
    "        # Extract\n",
    "        local_file_dir = extract(zip_filename)\n",
    "\n",
    "        # Transform\n",
    "        df = transform(local_file_dir)\n",
    "        dfs.append(df)\n",
    "\n",
    "    # Load\n",
    "    df = load(\n",
    "        dfs,\n",
    "        connector_dict,\n",
    "        num_proc_data_files,\n",
    "        stage_name,\n",
    "        table_name,\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a83f8016-7320-42a7-bcea-9f9dbdd7a0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = snowflake.connector.connect(**connector_dict_no_db)\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f67ade5-127a-4b96-b90e-bfaf2c385668",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_on</th>\n",
       "      <th>name</th>\n",
       "      <th>is_default</th>\n",
       "      <th>is_current</th>\n",
       "      <th>origin</th>\n",
       "      <th>owner</th>\n",
       "      <th>comment</th>\n",
       "      <th>options</th>\n",
       "      <th>retention_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-01-27 16:12:39.701000-08:00</td>\n",
       "      <td>DEMO_DB</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td></td>\n",
       "      <td>SYSADMIN</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-01-27 10:58:19.534000-08:00</td>\n",
       "      <td>SNOWFLAKE_SAMPLE_DATA</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>SFC_SAMPLES.SAMPLE_DATA</td>\n",
       "      <td>ACCOUNTADMIN</td>\n",
       "      <td>Provided by Snowflake during account provisioning</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-01-27 16:12:52.421000-08:00</td>\n",
       "      <td>UTIL_DB</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td></td>\n",
       "      <td>SYSADMIN</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        created_on                   name is_default  \\\n",
       "0 2022-01-27 16:12:39.701000-08:00                DEMO_DB          N   \n",
       "1 2022-01-27 10:58:19.534000-08:00  SNOWFLAKE_SAMPLE_DATA          N   \n",
       "2 2022-01-27 16:12:52.421000-08:00                UTIL_DB          N   \n",
       "\n",
       "  is_current                   origin         owner  \\\n",
       "0          N                               SYSADMIN   \n",
       "1          N  SFC_SAMPLES.SAMPLE_DATA  ACCOUNTADMIN   \n",
       "2          N                               SYSADMIN   \n",
       "\n",
       "                                             comment options retention_time  \n",
       "0                                                                         1  \n",
       "1  Provided by Snowflake during account provisioning                      1  \n",
       "2                                                                         1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.7 ms, sys: 404 Âµs, total: 14.1 ms\n",
      "Wall time: 123 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "query = \"\"\"\n",
    "        SHOW DATABASES\n",
    "        \"\"\"\n",
    "_ = show_sql_df(query, cur, None, True, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82f4ff55-1487-43c9-84bd-43b81c867466",
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a02a42-82ef-48f0-b6aa-de03cd24df89",
   "metadata": {},
   "source": [
    "## Create Database and Supporting Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f15541-08ad-4d18-b375-d5b15eca5ce3",
   "metadata": {},
   "source": [
    "The inspections data will be stored in a SQL database running on [Snowflake](https://www.snowflake.com/). This database will be created here along with any required supporting resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e614e53f-f3cb-43e2-9ffd-9300ea341c46",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_on</th>\n",
       "      <th>name</th>\n",
       "      <th>is_default</th>\n",
       "      <th>is_current</th>\n",
       "      <th>origin</th>\n",
       "      <th>owner</th>\n",
       "      <th>comment</th>\n",
       "      <th>options</th>\n",
       "      <th>retention_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-03-03 19:31:12.980000-08:00</td>\n",
       "      <td>DINESAFE</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td></td>\n",
       "      <td>SYSADMIN</td>\n",
       "      <td>Toronto dinesafe inspections</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        created_on      name is_default is_current origin  \\\n",
       "0 2022-03-03 19:31:12.980000-08:00  DINESAFE          N          Y          \n",
       "\n",
       "      owner                       comment options retention_time  \n",
       "0  SYSADMIN  Toronto dinesafe inspections                      1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created database dinesafe\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_on</th>\n",
       "      <th>name</th>\n",
       "      <th>database_name</th>\n",
       "      <th>schema_name</th>\n",
       "      <th>kind</th>\n",
       "      <th>comment</th>\n",
       "      <th>cluster_by</th>\n",
       "      <th>rows</th>\n",
       "      <th>bytes</th>\n",
       "      <th>owner</th>\n",
       "      <th>retention_time</th>\n",
       "      <th>automatic_clustering</th>\n",
       "      <th>change_tracking</th>\n",
       "      <th>search_optimization</th>\n",
       "      <th>search_optimization_progress</th>\n",
       "      <th>search_optimization_bytes</th>\n",
       "      <th>is_external</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-03-03 19:31:14.277000-08:00</td>\n",
       "      <td>INSPECTIONS</td>\n",
       "      <td>DINESAFE</td>\n",
       "      <td>PUBLIC</td>\n",
       "      <td>TABLE</td>\n",
       "      <td>Toronto Dinesafe inspections data</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>SYSADMIN</td>\n",
       "      <td>1</td>\n",
       "      <td>OFF</td>\n",
       "      <td>OFF</td>\n",
       "      <td>OFF</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        created_on         name database_name schema_name  \\\n",
       "0 2022-03-03 19:31:14.277000-08:00  INSPECTIONS      DINESAFE      PUBLIC   \n",
       "\n",
       "    kind                            comment cluster_by  rows  bytes     owner  \\\n",
       "0  TABLE  Toronto Dinesafe inspections data                0      0  SYSADMIN   \n",
       "\n",
       "  retention_time automatic_clustering change_tracking search_optimization  \\\n",
       "0              1                  OFF             OFF                 OFF   \n",
       "\n",
       "  search_optimization_progress search_optimization_bytes is_external  \n",
       "0                         None                      None           N  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created table inspections\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_on</th>\n",
       "      <th>name</th>\n",
       "      <th>database_name</th>\n",
       "      <th>schema_name</th>\n",
       "      <th>type</th>\n",
       "      <th>owner</th>\n",
       "      <th>comment</th>\n",
       "      <th>format_options</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-03-03 19:31:15.501000-08:00</td>\n",
       "      <td>COMMACOLSEP_ONEHEADROW</td>\n",
       "      <td>DINESAFE</td>\n",
       "      <td>PUBLIC</td>\n",
       "      <td>CSV</td>\n",
       "      <td>SYSADMIN</td>\n",
       "      <td>file format for Toronto dinesafe processed data files</td>\n",
       "      <td>{\"TYPE\":\"CSV\",\"RECORD_DELIMITER\":\"\\n\",\"FIELD_DELIMITER\":\",\",\"FILE_EXTENSION\":null,\"SKIP_HEADER\":1,\"DATE_FORMAT\":\"AUTO\",\"TIME_FORMAT\":\"AUTO\",\"TIMESTAMP_FORMAT\":\"AUTO\",\"BINARY_FORMAT\":\"HEX\",\"ESCAPE\"...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        created_on                    name database_name  \\\n",
       "0 2022-03-03 19:31:15.501000-08:00  COMMACOLSEP_ONEHEADROW      DINESAFE   \n",
       "\n",
       "  schema_name type     owner  \\\n",
       "0      PUBLIC  CSV  SYSADMIN   \n",
       "\n",
       "                                                 comment  \\\n",
       "0  file format for Toronto dinesafe processed data files   \n",
       "\n",
       "                                                                                                                                                                                            format_options  \n",
       "0  {\"TYPE\":\"CSV\",\"RECORD_DELIMITER\":\"\\n\",\"FIELD_DELIMITER\":\",\",\"FILE_EXTENSION\":null,\"SKIP_HEADER\":1,\"DATE_FORMAT\":\"AUTO\",\"TIME_FORMAT\":\"AUTO\",\"TIMESTAMP_FORMAT\":\"AUTO\",\"BINARY_FORMAT\":\"HEX\",\"ESCAPE\"...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created file format COMMACOLSEP_ONEHEADROW\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_on</th>\n",
       "      <th>name</th>\n",
       "      <th>database_name</th>\n",
       "      <th>schema_name</th>\n",
       "      <th>url</th>\n",
       "      <th>has_credentials</th>\n",
       "      <th>has_encryption_key</th>\n",
       "      <th>owner</th>\n",
       "      <th>comment</th>\n",
       "      <th>region</th>\n",
       "      <th>type</th>\n",
       "      <th>cloud</th>\n",
       "      <th>notification_channel</th>\n",
       "      <th>storage_integration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-03-03 19:31:16.442000-08:00</td>\n",
       "      <td>PROCESSED_DINESAFE_DATA</td>\n",
       "      <td>DINESAFE</td>\n",
       "      <td>PUBLIC</td>\n",
       "      <td></td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>SYSADMIN</td>\n",
       "      <td>local dir for Toronto dinesafe processed data</td>\n",
       "      <td>None</td>\n",
       "      <td>INTERNAL</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        created_on                     name database_name  \\\n",
       "0 2022-03-03 19:31:16.442000-08:00  PROCESSED_DINESAFE_DATA      DINESAFE   \n",
       "\n",
       "  schema_name url has_credentials has_encryption_key     owner  \\\n",
       "0      PUBLIC                   N                  N  SYSADMIN   \n",
       "\n",
       "                                         comment region      type cloud  \\\n",
       "0  local dir for Toronto dinesafe processed data   None  INTERNAL  None   \n",
       "\n",
       "  notification_channel storage_integration  \n",
       "0                 None                None  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created stage processed_dinesafe_data\n",
      "CPU times: user 591 ms, sys: 6.81 ms, total: 598 ms\n",
      "Wall time: 4.68 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "create_database(connector_dict_no_db, DB_NAME)\n",
    "create_table(connector_dict, table_name)\n",
    "create_file_format(connector_dict, file_format_name)\n",
    "create_stage(connector_dict, stage_name, file_format_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab261f9-0503-43b5-832c-4841aa284579",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Get Data and Populate Database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444f1d1e-e8b5-4d60-b8c6-b56228b25c3c",
   "metadata": {},
   "source": [
    "Retrieve DineSafe program data snapshots from WayBackMachine and append to the `dinesafe` database (drop duplicate inspections and change data types before appending to table)\n",
    "A helper function `retrieve_data()` is used to\n",
    "- download historical inspections data from WayBackMachine (`extract()`) and unzip contents into `data/raw`\n",
    "- process the raw inspections data (`process_data()`)\n",
    "  - change `INSPECTION_DATE` to a `datetime`\n",
    "  - change `MINIMUM_INSPECTIONS_PERYEAR` to an integer datatype\n",
    "  - clean `AMOUNT_FINED` (remove commas) and convert to numerical datatype\n",
    "  - append `LATITUDE` and `LONGITUDE` columns (if not present)\n",
    "    - some inspections data files have these but others don't\n",
    "    - since we'll be appending all files to the same database table, they will all need to have these columns even if the columns contain missing values\n",
    "  - append `LATITUDE` and `LONGITUDE` columns to lowercase\n",
    "- append the processed data to the `inspections` table in the `dinesafe` SQL database (`load()`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35963dde-977d-466c-b34e-13d377b415e9",
   "metadata": {},
   "source": [
    "Run the ETL workflow to retrieve historical inspections data files, process each file and append processed data to the `inspections` table of the `dinesafe` database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff6c456b-6a91-4615-9bd2-74b41dba90ba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from 20130723222156...Processing data from data/raw/20130723222156...Done.\n",
      "Downloading data from 20150603085055...Processing data from data/raw/20150603085055...Done.\n",
      "Downloading data from 20151012004454...Processing data from data/raw/20151012004454...Done.\n",
      "Downloading data from 20160129205023...Processing data from data/raw/20160129205023...Done.\n",
      "Downloading data from 20160317045436...Processing data from data/raw/20160317045436...Done.\n",
      "Downloading data from 20160915001010...Processing data from data/raw/20160915001010...Done.\n",
      "Downloading data from 20170303162206...Processing data from data/raw/20170303162206...Done.\n",
      "Downloading data from 20170330001043...Processing data from data/raw/20170330001043...Done.\n",
      "Downloading data from 20170726115444...Processing data from data/raw/20170726115444...Done.\n",
      "Downloading data from 20190116215713...Processing data from data/raw/20190116215713...Done.\n",
      "Downloading data from 20190126084933...Processing data from data/raw/20190126084933...Done.\n",
      "Downloading data from 20190614092848...Processing data from data/raw/20190614092848...Done.\n",
      "Downloading data from 20210626163552...Processing data from data/raw/20210626163552...Done.\n",
      "Will split processed data among 10 CSV files.\n",
      "Exported processed data to CSV file index 0.\n",
      "Exported processed data to CSV file index 1.\n",
      "Exported processed data to CSV file index 2.\n",
      "Exported processed data to CSV file index 3.\n",
      "Exported processed data to CSV file index 4.\n",
      "Exported processed data to CSV file index 5.\n",
      "Exported processed data to CSV file index 6.\n",
      "Exported processed data to CSV file index 7.\n",
      "Exported processed data to CSV file index 8.\n",
      "Exported processed data to CSV file index 9.\n",
      "PUT file://data/processed/dinesafe_1.csv @processed_dinesafe_data...Done.\n",
      "PUT file://data/processed/dinesafe_8.csv @processed_dinesafe_data...Done.\n",
      "PUT file://data/processed/dinesafe_2.csv @processed_dinesafe_data...Done.\n",
      "PUT file://data/processed/dinesafe_5.csv @processed_dinesafe_data...Done.\n",
      "PUT file://data/processed/dinesafe_9.csv @processed_dinesafe_data...Done.\n",
      "PUT file://data/processed/dinesafe_0.csv @processed_dinesafe_data...Done.\n",
      "PUT file://data/processed/dinesafe_6.csv @processed_dinesafe_data...Done.\n",
      "PUT file://data/processed/dinesafe_3.csv @processed_dinesafe_data...Done.\n",
      "PUT file://data/processed/dinesafe_4.csv @processed_dinesafe_data...Done.\n",
      "PUT file://data/processed/dinesafe_7.csv @processed_dinesafe_data...Done.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>size</th>\n",
       "      <th>md5</th>\n",
       "      <th>last_modified</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>processed_dinesafe_data/dinesafe_0.csv.gz</td>\n",
       "      <td>1931184</td>\n",
       "      <td>e8c5a1e2f26c38175ea13adf898249bd</td>\n",
       "      <td>Fri, 4 Mar 2022 03:34:41 GMT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>processed_dinesafe_data/dinesafe_1.csv.gz</td>\n",
       "      <td>1936448</td>\n",
       "      <td>abea34927c3ec983697426116aa2ee52</td>\n",
       "      <td>Fri, 4 Mar 2022 03:34:33 GMT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>processed_dinesafe_data/dinesafe_2.csv.gz</td>\n",
       "      <td>1955840</td>\n",
       "      <td>ac4a430d01f7d05095760fcb853fecb5</td>\n",
       "      <td>Fri, 4 Mar 2022 03:34:36 GMT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>processed_dinesafe_data/dinesafe_3.csv.gz</td>\n",
       "      <td>1983520</td>\n",
       "      <td>1b88643766c530410e01ea88e8415fd2</td>\n",
       "      <td>Fri, 4 Mar 2022 03:34:44 GMT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>processed_dinesafe_data/dinesafe_4.csv.gz</td>\n",
       "      <td>2009360</td>\n",
       "      <td>b9c277b6e5fb6abab0a81ac9a328289e</td>\n",
       "      <td>Fri, 4 Mar 2022 03:34:45 GMT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>processed_dinesafe_data/dinesafe_5.csv.gz</td>\n",
       "      <td>2024224</td>\n",
       "      <td>e33409eb61fb75a7230a0b097d1aac3b</td>\n",
       "      <td>Fri, 4 Mar 2022 03:34:38 GMT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>processed_dinesafe_data/dinesafe_6.csv.gz</td>\n",
       "      <td>2031328</td>\n",
       "      <td>e9dbb529001e56e0e46a1826dbbaf143</td>\n",
       "      <td>Fri, 4 Mar 2022 03:34:43 GMT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>processed_dinesafe_data/dinesafe_7.csv.gz</td>\n",
       "      <td>2324064</td>\n",
       "      <td>2c9546bd365f45f1a8ed9c85415c294d</td>\n",
       "      <td>Fri, 4 Mar 2022 03:34:47 GMT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>processed_dinesafe_data/dinesafe_8.csv.gz</td>\n",
       "      <td>2474688</td>\n",
       "      <td>e18b67df04951942ca5d21cb1e66d59c</td>\n",
       "      <td>Fri, 4 Mar 2022 03:34:35 GMT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>processed_dinesafe_data/dinesafe_9.csv.gz</td>\n",
       "      <td>2507424</td>\n",
       "      <td>1d4ef3b336bed2e304b50756b1fd5e29</td>\n",
       "      <td>Fri, 4 Mar 2022 03:34:40 GMT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        name     size  \\\n",
       "0  processed_dinesafe_data/dinesafe_0.csv.gz  1931184   \n",
       "1  processed_dinesafe_data/dinesafe_1.csv.gz  1936448   \n",
       "2  processed_dinesafe_data/dinesafe_2.csv.gz  1955840   \n",
       "3  processed_dinesafe_data/dinesafe_3.csv.gz  1983520   \n",
       "4  processed_dinesafe_data/dinesafe_4.csv.gz  2009360   \n",
       "5  processed_dinesafe_data/dinesafe_5.csv.gz  2024224   \n",
       "6  processed_dinesafe_data/dinesafe_6.csv.gz  2031328   \n",
       "7  processed_dinesafe_data/dinesafe_7.csv.gz  2324064   \n",
       "8  processed_dinesafe_data/dinesafe_8.csv.gz  2474688   \n",
       "9  processed_dinesafe_data/dinesafe_9.csv.gz  2507424   \n",
       "\n",
       "                                md5                 last_modified  \n",
       "0  e8c5a1e2f26c38175ea13adf898249bd  Fri, 4 Mar 2022 03:34:41 GMT  \n",
       "1  abea34927c3ec983697426116aa2ee52  Fri, 4 Mar 2022 03:34:33 GMT  \n",
       "2  ac4a430d01f7d05095760fcb853fecb5  Fri, 4 Mar 2022 03:34:36 GMT  \n",
       "3  1b88643766c530410e01ea88e8415fd2  Fri, 4 Mar 2022 03:34:44 GMT  \n",
       "4  b9c277b6e5fb6abab0a81ac9a328289e  Fri, 4 Mar 2022 03:34:45 GMT  \n",
       "5  e33409eb61fb75a7230a0b097d1aac3b  Fri, 4 Mar 2022 03:34:38 GMT  \n",
       "6  e9dbb529001e56e0e46a1826dbbaf143  Fri, 4 Mar 2022 03:34:43 GMT  \n",
       "7  2c9546bd365f45f1a8ed9c85415c294d  Fri, 4 Mar 2022 03:34:47 GMT  \n",
       "8  e18b67df04951942ca5d21cb1e66d59c  Fri, 4 Mar 2022 03:34:35 GMT  \n",
       "9  1d4ef3b336bed2e304b50756b1fd5e29  Fri, 4 Mar 2022 03:34:40 GMT  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 10 processed data files to stage processed_dinesafe_data\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_rows</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>981799</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   num_rows\n",
       "0    981799"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied 1 rows of processed data from stage processed_dinesafe_data to table inspections\n",
      "CPU times: user 1min 16s, sys: 3.3 s, total: 1min 19s\n",
      "Wall time: 3min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = retrieve_data(zip_filenames, connector_dict, num_proc_data_files, stage_name, table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f64498-ba11-4578-a536-16f4f4bdc37e",
   "metadata": {},
   "source": [
    "These datasets list each infraction for a single inspection on a separate row. We will now need to filter these infractions to only select relevant ones and then aggregate them by inspection, since each row (inspection) will be used as an independent observation by the ML model we train later.\n",
    "\n",
    "In the next notebook (`2_sql_filter_transform_v2.ipynb`), we will filter these infractions and aggregate them by inspection."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
